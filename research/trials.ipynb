{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\CodeCommand\\\\End to End Medical Chatbot Project Implementation  Generative AI  English  Euron'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    }
   ],
   "source": [
    "extracted_data = load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents=extracted_data)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 557\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazbeen Ai\\AppData\\Local\\Temp\\ipykernel_13588\\4238859041.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings= download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY=os.environ.get(key='PINECONE_API_KEY')\n",
    "GROQ_API_KEY=os.environ.get(key='GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"medicalbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"medicalbot-m67xe5j.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = 'medicalbot'\n",
    "\n",
    "pc.create_index(\n",
    "  name=index_name,\n",
    "  dimension=384,\n",
    "  metric=\"cosine\",\n",
    "  spec=ServerlessSpec(\n",
    "    cloud=\"aws\",\n",
    "    region=\"us-east-1\"\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1f51db30430>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load Existing index\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name,\n",
    "    embedding=embeddings\n",
    "    )\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs = retriever.invoke('what is DeeperGoogLeNet on Tiny ImageNet?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='17f2d0f8-cbbe-4125-a3fd-eb63bbfb66f0', metadata={'author': 'Adrian Rosebrock', 'creationdate': '2017-11-29T09:00:19-05:00', 'creator': 'calibre (3.39.1) [https://calibre-ebook.com]', 'keywords': '00 Важно', 'moddate': '2019-11-02T16:27:44+03:00', 'page': 8.0, 'page_label': '7', 'producer': 'pdfTeX-1.40.16', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) kpathsea version 6.2.1', 'source': 'Data\\\\deep-learning-for-computer-vision-with-python-starter-bundle_compress.pdf', 'subject': '', 'title': 'Deep Learning for Computer Vision with Python. 2 - Practitioner Bundle', 'total_pages': 210.0, 'trapped': '/False'}, page_content='11.2.5 MiniGoogLeNet: Experiment #3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\n11.3 The Tiny ImageNet Challenge 146\\n11.3.1 Downloading Tiny ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n11.3.2 The Tiny ImageNet Directory Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n11.3.3 Building the Tiny ImageNet Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n11.4 DeeperGoogLeNet on Tiny ImageNet 153\\n11.4.1 Implementing DeeperGoogLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n11.4.2 Training DeeperGoogLeNet on Tiny ImageNet . . . . . . . . . . . . . . . . . . . . . . . . 161\\n11.4.3 Creating the Training Script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n11.4.4 Creating the Evaluation Script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163'),\n",
       " Document(id='d1aa1348-767e-4824-8999-0cb948e97d14', metadata={'author': 'Adrian Rosebrock', 'creationdate': '2017-11-29T09:00:19-05:00', 'creator': 'calibre (3.39.1) [https://calibre-ebook.com]', 'keywords': '00 Важно', 'moddate': '2019-11-02T16:27:44+03:00', 'page': 158.0, 'page_label': '157', 'producer': 'pdfTeX-1.40.16', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) kpathsea version 6.2.1', 'source': 'Data\\\\deep-learning-for-computer-vision-with-python-starter-bundle_compress.pdf', 'subject': '', 'title': 'Deep Learning for Computer Vision with Python. 2 - Practitioner Bundle', 'total_pages': 210.0, 'trapped': '/False'}, page_content='11.4 DeeperGoogLeNet on Tiny ImageNet 157\\nand num3x3 variables, respectively:\\n47 # define the second branch of the Inception module which\\n48 # consists of 1x1 and 3x3 convolutions\\n49 second = DeeperGoogLeNet.conv_module(x, num3x3Reduce, 1, 1,\\n50 (1, 1), chanDim, reg=reg, name=stage + \"_second1\")\\n51 second = DeeperGoogLeNet.conv_module(second, num3x3, 3, 3,\\n52 (1, 1), chanDim, reg=reg, name=stage + \"_second2\")\\nHere we can see that the ﬁrst conv_module applies the 1 ×1 convolutions to the input. The\\noutput of these 1 ×1 convolutions are then passed into the second conv_module which performs a\\nseries of 3 ×3 convolutions. The number of 1 ×1 convolutions is always smaller than the number\\nof 3 ×3 convolutions, thereby serving as a form of dimensionality reduction.\\nThe third branch in Inception is identical to the second branch, only instead of performing a\\n1×1 reduce followed by a 3×3 expand, we are now going to use a1×1 reduce and a 5×5 expand:'),\n",
       " Document(id='a9fb6337-3ba0-42b7-9306-35a6c8d4eacf', metadata={'author': 'Adrian Rosebrock', 'creationdate': '2017-11-29T09:00:19-05:00', 'creator': 'calibre (3.39.1) [https://calibre-ebook.com]', 'keywords': '00 Важно', 'moddate': '2019-11-02T16:27:44+03:00', 'page': 155.0, 'page_label': '154', 'producer': 'pdfTeX-1.40.16', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) kpathsea version 6.2.1', 'source': 'Data\\\\deep-learning-for-computer-vision-with-python-starter-bundle_compress.pdf', 'subject': '', 'title': 'Deep Learning for Computer Vision with Python. 2 - Practitioner Bundle', 'total_pages': 210.0, 'trapped': '/False'}, page_content='154 Chapter 11. GoogLeNet\\nFigure 11.9: Our modiﬁed GoogLeNet architecture which we will call “DeeperGoogLeNet”.\\nThe DeeperGoogLNet architecture is identical to the original GoogLeNet architecture with two\\nmodiﬁcations: (1) 5 ×5 ﬁlters with a stride of 1 ×1 are used in the ﬁrst CONV layer and (2) the ﬁnal\\ntwo inception modules (5a and 5b) are left out.\\n--- pyimagesearch\\n| |--- __init__.py\\n| |--- callbacks\\n| |--- io\\n| |--- nn\\n| | |--- __init__.py\\n| | |--- conv\\n| | | |--- __init__.py\\n| | | |--- alexnet.py\\n| | | |--- deepergooglenet.py\\n| | | |--- lenet.py\\n| | | |--- minigooglenet.py\\n| | | |--- minivggnet.py\\n| | | |--- fcheadnet.py\\n| | | |--- shallownet.py\\n| |--- preprocessing\\n| |--- utils\\nFrom there, we can start working on the implementation:\\n1 # import the necessary packages\\n2 from keras.layers.normalization import BatchNormalization\\n3 from keras.layers.convolutional import Conv2D\\n4 from keras.layers.convolutional import AveragePooling2D'),\n",
       " Document(id='980b60f5-f473-413e-ab59-7c06296996a6', metadata={'author': 'Adrian Rosebrock', 'creationdate': '2017-11-29T09:00:19-05:00', 'creator': 'calibre (3.39.1) [https://calibre-ebook.com]', 'keywords': '00 Важно', 'moddate': '2019-11-02T16:27:44+03:00', 'page': 148.0, 'page_label': '147', 'producer': 'pdfTeX-1.40.16', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) kpathsea version 6.2.1', 'source': 'Data\\\\deep-learning-for-computer-vision-with-python-starter-bundle_compress.pdf', 'subject': '', 'title': 'Deep Learning for Computer Vision with Python. 2 - Practitioner Bundle', 'total_pages': 210.0, 'trapped': '/False'}, page_content='11.3 The Tiny ImageNet Challenge 147\\nLarge Scale Visual Recognition Challenge (ILSVRC) have varying widths and heights. Therefore,\\nwhenever we work with ILSVRC, we ﬁrst need to resize all images in the dataset to a ﬁxed width\\nand height before we can train our network. To help students focus strictly on the deep learning and\\nimage classiﬁcation component (and not get caught up in image processing details), all images in\\nthe Tiny ImageNet dataset have been resized to 64×64 pixels and center cropped.\\nIn some ways, having the images resized makes Tiny ImageNet a bit more challenging than\\nit’s bigger brother, ILSVRC. In ILSVRC we are free to apply any type of resizing, cropping, etc.\\noperations that we see ﬁt. However, with Tiny ImageNet, much of the image has already been\\ndiscarded for us. As we’ll ﬁnd out, obtaining a reasonable rank-1 and rank-5 accuracy on Tiny\\nImageNet isn’t as easy as one might think, making it a great, insightful dataset for budding deep'),\n",
       " Document(id='ae2b565f-6ed3-45ca-b5cd-ee31f209df6b', metadata={'author': 'Adrian Rosebrock', 'creationdate': '2017-11-29T09:00:19-05:00', 'creator': 'calibre (3.39.1) [https://calibre-ebook.com]', 'keywords': '00 Важно', 'moddate': '2019-11-02T16:27:44+03:00', 'page': 162.0, 'page_label': '161', 'producer': 'pdfTeX-1.40.16', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) kpathsea version 6.2.1', 'source': 'Data\\\\deep-learning-for-computer-vision-with-python-starter-bundle_compress.pdf', 'subject': '', 'title': 'Deep Learning for Computer Vision with Python. 2 - Practitioner Bundle', 'total_pages': 210.0, 'trapped': '/False'}, page_content='11.4 DeeperGoogLeNet on Tiny ImageNet 161\\n11.4.2 Training DeeperGoogLeNet on Tiny ImageNet\\nNow that our DeeperGoogLeNet architecture is implemented, we need to create a Python script\\nthat will train the network on Tiny ImageNet. We’ll also need to create a second Python script that\\nwill be responsible for evaluating our model on the testing set by computing rank-1 and rank-5\\naccuracies.\\nOnce we have completed both of these tasks, I’ll share three experiments I ran when gathering\\nthe results for this chapter. These experiments will form a “case study” and enable you to learn\\nhow to run an experiment, investigate the results, and make an educated guess on how to tune your\\nhyperparameters to obtain a better performing network in your next experiment.\\n11.4.3 Creating the Training Script\\nLet’s go ahead and implement the training script – open up a new ﬁle, name ittrain.py, and insert\\nthe following code:\\n1 # set the matplotlib backend so figures can be saved in the background')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0.4,\n",
    "    max_tokens=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an intelligent AI assistant designed to provide accurate and relevant responses based on retrieved knowledge. Your primary goal is to assist users by leveraging the available information effectively.\n",
    "\n",
    "Guidelines:\n",
    "- If the retrieved documents contain relevant information, provide a well-structured and concise answer.\n",
    "- If the retrieved data is insufficient or unavailable, respond professionally:\n",
    "  **\"I'm sorry, but I don't have enough information to answer that.\"**\n",
    "- Maintain clarity, professionalism, and a helpful tone in all responses.\n",
    "- Avoid making assumptions or generating misleading information.\n",
    "- When necessary, encourage users to refine their query for better results.\n",
    "\n",
    "Your responses should always be structured, informative, and aligned with the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain =  create_stuff_documents_chain(llm, prompt_template)\n",
    "rag_chain =  create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fine-tuning is a type of transfer learning that involves modifying and '\n",
      " 're-training parts of a deep learning model that has already been trained on '\n",
      " 'a specific dataset. Typically, these networks are state-of-the-art '\n",
      " 'architectures such as VGG, ResNet, and Inception, which have been trained on '\n",
      " 'the ImageNet dataset.\\n'\n",
      " '\\n'\n",
      " 'During fine-tuning, the final set of layers (the \"head\") is removed from the '\n",
      " 'network, and the body of the original network is frozen while new '\n",
      " 'fully-connected (FC) layers are trained. Once the network starts to obtain '\n",
      " 'reasonable accuracy, part (if not all) of the body is unfrozen, allowing '\n",
      " 'training to continue. This technique leverages pre-existing network '\n",
      " 'architectures and their rich, discriminative set of filters, enabling faster '\n",
      " 'learning and higher accuracy transfer learning models with less effort than '\n",
      " 'training from scratch.\\n'\n",
      " '\\n'\n",
      " 'Before performing network surgery, it is essential to know the layer names '\n",
      " 'and indexes of every layer in a given deep learning model. This information '\n",
      " 'is necessary for freezing and unfreezing specific layers in a pre-trained '\n",
      " 'convolutional neural network (CNN). Properly executed, fine-tuning will '\n",
      " 'nearly always result in higher accuracy.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "response = rag_chain.invoke({\n",
    "    \"input\": \"What is Fine-tuning Networks?\"\n",
    "})\n",
    "pprint.pp(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
